version: "3.9"

services:
  llama-server:
    build: .
    image: atlas/llama-cpp-server:latest
    container_name: llama-cpp-server
    restart: unless-stopped
    environment:
      MODEL_PATH: ${MODEL_PATH:-/models/model.gguf}
      HOST_PORT: ${HOST_PORT:-8081}
      CONTEXT_SIZE: ${CONTEXT_SIZE:-4096}
      LLAMA_THREADS: ${LLAMA_THREADS:-4}
      LLAMA_API_KEY: ${LLAMA_API_KEY:-}
    ports:
      - "${HOST_PORT:-8081}:8081"
    volumes:
      - ${MODEL_DIR:-./models}:/models:ro

